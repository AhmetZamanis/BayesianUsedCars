---
title: "Bayesian Linear Regression - Used Cars Analysis"
author: "Ahmet Zamanis"
output: 
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(messsage = FALSE)

library(rmarkdown)

library(tidyverse) 
library(caTools) 
library(GGally)
library(fitdistrplus) 
library(car)
library(forecast)

library(ggplot2) 
library(patchwork) 
library(RColorBrewer) 
library(ggthemes) 
library(gt)


library(ggfortify) 
library(broom)
library(rstanarm) 
library(bayesplot) 
library(bayestestR) 
library(insight)
library(purrr)

options(scipen=999)
```

## Introduction

Bayesian Linear Regression is an alternative approach to linear regression that is based on Bayesian inference, and the Bayes Theorem. While the better known Ordinary Least Squares approach to linear regression estimates a single value for the regression model parameters, the Bayesian approach generates a probability distribution of possible values for each model parameter. BLR offers some benefits and additional insight compared to the OLS method:

- We can more easily quantify and visualize the uncertainty in the model, since the output is a range of values for the model parameters, instead of single values. A more spread out probability distribution for the parameters will mean our model has more uncertainty.
- If we have knowledge or guesses about the prior distributions of model parameters, we can use them in building the model as informative prior distributions.

In this example analysis, we have a dataset of used cars, which includes their prices and various features. We will try to predict the price of used cars, first using regular OLS linear regression, then with BLR. We will compare their results, and analyze the additional information provided by BLR.
\
\
The dataset was sourced from [Kaggle](https://www.kaggle.com/datasets/nehalbirla/vehicle-dataset-from-cardekho), provided by the contributors [Nehal Birla](https://www.kaggle.com/nehalbirla), [Nishant Verma](https://www.kaggle.com/nishantverma02) and [Nikhil Kushawa](https://www.kaggle.com/kushbun27).

## Data Preparation

Let's load our dataset and view the first 5 rows, along with simple summary statistics.
```{r}
df_org <- read.csv("car_data.csv", header=TRUE, encoding="UTF-8" ) 

#factor conversions
df_org$fuel <- as.factor(df_org$fuel)
df_org$seller_type <- as.factor(df_org$seller_type)
df_org$transmission <- as.factor(df_org$transmission)
df_org$owner <- as.factor(df_org$owner)

#owner is ordinal categorical
df_org$owner <- factor(df_org$owner, levels=c("Test Drive Car", "First Owner", "Second Owner",
                                              "Third Owner", "Fourth & Above Owner"))
```
\
```{r}
tb1 <- gt(data=df_org[1:5,], rownames_to_stub = TRUE) %>% 
  tab_header(title="Used cars dataset") %>% 
  opt_table_font(font=list(google_font("Calibri"), default_fonts())) %>% 
  tab_style(locations=cells_column_labels(columns=everything()), 
            style=list(
              cell_borders(sides="bottom", weight=px(3)), 
              cell_text(weight="bold"))) 

tb1  
```
\
```{r}
summary(df_org)
```
The original dataset has 4340 observations, 8 columns. The variables in the original dataset are:

- name: Name of the car model. Character variable.
- year: Year of the car model. Numeric variable.
- selling_price: Selling price of the car. Numeric variable. The outcome variable we will try to predict.
- km_driven: The distance traveled by the car, in kilometers. Numeric variable.
- fuel: The fuel type of the car. Factor variable with five levels.
- seller_type: The car's seller. Factor variable with three levels.
- transmission: The car's transmission type. Factor variable with two levels.
- owner: Number of previous owners. Factor variable with five levels.

There are some data cleaning and recoding steps to consider:

- Selling price and kilometers driven are both large numbers, in hundreds of thousands and millions. Let's divide both by 1000 to get them in units of thousands.
- The fuel types CNG, LPG and Electric have very few observations, compared to Petrol and Diesel which have almost the same number of observations. We may want to drop the fuel categories besides Petrol and Diesel, to achieve a simpler, almost perfectly balanced variable.
- Similarly, there are few observations with the "Trustmark Dealer" seller type. We can recode these as "Dealer" type observations.
- There are few observations with "Test Drive Car" and "Fourth & Above Owner" levels in the owner variable. Furthermore, without Test Drive Cars, number of owners would be a naturally ordered factor variable. We can drop observations with "Test Drive Car" and "Fourth & Above Owner", along with these levels of the owner variable.

```{r}
df_org$selling_price <- df_org$selling_price/1000
df_org$km_driven <- df_org$km_driven/1000

#get rid of non-petrol/diesel obs
df_org <- subset(df_org, fuel=="Diesel" | fuel=="Petrol")
df_org$fuel <- droplevels(df_org$fuel)

#combine dealers and trustmark dealers
df_org$seller_type[df_org$seller_type=="Trustmark Dealer"] = "Dealer"
df_org$seller_type <- droplevels(df_org$seller_type)

#drop test drive cars and fourth+ owner cars
df_org <- subset(df_org, owner!="Test Drive Car" & owner!="Fourth & Above Owner")
df_org$owner <- droplevels(df_org$owner)

summary(df_org)
```
With these changes, we have a dataset of 4182 variables, and simplified, less unbalanced factor variables.

## Exploratory Analysis

### Distributions

```{r}
for (i in 2:4) {
  x_val <- df_org[,i]
  x_lab <- colnames(df_org)[i]
  medi <- median(df_org[,i])
  mea <- mean(df_org[,i])
  q25 <- quantile(df_org[,i], 0.25)
  q75 <- quantile(df_org[,i], 0.75)
  
  p <- ggplot(df_org, aes(x=!!x_val)) + 
    geom_histogram(fill="#92C5DE", color="#92C5DE", size=1, alpha=0.5, bins = 30) +
    geom_point(aes(x=!!medi, y=0, color="#CA0020"), size=1) +
    geom_point(aes(x=!!q25, y=0, color="#FDB863"), size=1) +
    geom_point(aes(x=!!q75, y=0, color="#FDB863"), size=1) +
    geom_vline(xintercept=medi, color="#CA0020", size=1, linetype="dashed") +
    geom_vline(xintercept=q25, color="#FDB863", size=1, linetype="dashed") +
    geom_vline(xintercept=q75, color="#FDB863", size=1, linetype="dashed") +
    labs(x=x_lab, y="Count") +
    scale_color_identity(guide="legend",
                       name="Legend",
                       breaks=c("#CA0020", "#FDB863"),
                       labels=c("Median","1st-3rd quartiles")) +
    theme_bw() 
  
  assign(paste0("hist", i), p)
}
```
Let's look at the distributions of our numeric variables.
\
```{r}
hist3 + 
  scale_x_continuous(breaks=c(370,seq(2500,7500,2500))) + 
  labs(title="Distribution of selling_price", subtitle="N=4,182", x="selling_price, in thousands")
```
\
Our outcome variable, selling price, follows a very right skewed distribution. There are numerous very large outliers compared to the median and the interquartile range.

- While the minimum value is 20k, and the median 370k, the maximum is 8.9m, with numerous observations at a price level higher than 1m.
  - It may be sensible to remove the very high priced cars from the dataset, and try to predict the prices of more "ordinary" cars, as different price segments are likely to have different dynamics. 
  - However, since we want a model with some uncertainty to demonstrate the merits of BLR, we will keep the entire dataset for this analysis.

```{r}
((hist2 + scale_x_continuous(breaks=c(2000,2005,2011,2014,2016,2020))) / 
   (hist4 + labs(x="km_driven, in thousands") + scale_x_continuous(breaks=c(60,seq(200,800,200))))) + 
  plot_layout(guides="collect" ) + 
  plot_annotation(title="Distributions of year and km_driven", subtitle="N=4,182", theme=theme_bw())
```
\

- The year variable is left skewed, with a median year of 2014. Most cars in our dataset are newer models, roughly after 2010. Very few cars are from before 2005. Half of the cars are from 2011-2016.
- The km_driven variable is very right skewed, with a median of 60k, and a maximum of 806k. 
  - There are numerous cars with hundreds of thousands of kilometers driven. One observation has only 1 kilometer driven.
  
```{r}
for (i in 5:8) {
  df_bar <- as.data.frame(sort(table(df_org[,i]), decreasing=T))
  names(df_bar)[1] <- names(df_org)[i]
  names(df_bar)[2] <- "Count"
  x_val <- df_bar[,1]
  y_val <- df_bar[,2]
  y_lab <- colnames(df_bar)[2]
  pcts <- scales::percent(round(df_bar[,2] / sum(df_bar[,2]), 2))
  
  p <- ggplot(df_bar, aes(x=!!x_val, y=!!y_val)) + 
    geom_bar(stat="identity", color="#92C5DE", fill="#92C5DE", size=1, alpha=0.75, width=0.3) + 
    theme_bw() +
    labs(x="", y=y_lab) +
    geom_text(aes(label=!!pcts), vjust=-0.5, color="black", size=3)
  
  assign(paste0("bar", i), p)
}
```
Let's look at the balances of our factor variables.
\
```{r}
bar5 <- bar5 + scale_y_continuous(limits=c(0,3000)) + labs(x="Fuel type")
bar8 <- bar8 + scale_y_continuous(limits=c(0,3000)) + labs(x="Previous owners")
bar7 <- bar7 + scale_y_continuous(limits=c(0,4000)) + labs(x="Seller type")
bar6 <- bar6 + scale_y_continuous(limits=c(0,4000)) + labs(x="Transmission type")

(bar5 | bar8) / (bar6 | bar7) + plot_annotation(title="Balances of factor variables", subtitle="N=4,182", 
                                 theme=theme_bw())
```
\

- Fuel type is balanced almost perfectly between diesel and petrol.
- Previous owners is unbalanced, most cars are from first owners, and very few are from third owners.
- Seller type is unbalanced, most cars are from individual sellers.
- Transmission type is very unbalanced, most cars are manual transmission cars.

### Correlations

Let's check the correlations between our variables, starting with the numeric variables.
\
```{r}
ggpairs(df_org[,2:4])
```
\

- There is a moderate, positive correlation of 0.4 between year and selling price. The coefficient is highly statistically significant. 
  - The scatterplot suggests a non-linear, exponential-like relationship. We can see some outliers with much higher selling prices compared to observations in their year. Again, these are the luxury cars with millions in selling price, likely subject to different pricing dynamics.
- There is a moderate, negative correlation of -0.42 between year and km_driven. The coefficient is highly statistically significant.
  - The scatterplot suggests a non-linear relationship, where kilometers driven increases from 1995 to around 2010, and declines again after 2010. 
  - This is likely due to a lack of observations from older years, as most cars in our dataset are from 2010 and onwards. Intuitively, we would expect older cars to have higher kilometers driven. It's possible that older cars with higher kilometers are not put up for sale.
- The correlation between kilometers driven and selling price is weak, with a negative coefficient of -0.18. The coefficient is highly statistically significant.
  - Before a selling price of roughly 1.25m, we have some cars with kilometers driven above 200k. But after a selling price of roughly 1.25m, all cars are below 100k kilometers. 
  - It's likely that very high-priced, luxury or classic cars are driven less than ordinary, daily driver cars.
  
\
Let's check the degrees of association between our factor variables, with tile plots and chi-square tests.
\
```{r}
tchi1 <- table(df_org$fuel, df_org$seller_type)
dfchi1 <- as.data.frame(tchi1)
names(dfchi1)[1] <- "fuel"
names(dfchi1)[2] <- "seller_type"
names(dfchi1)[3] <- "Count"
chi1 <- chisq.test(tchi1)

ht1 <- ggplot(dfchi1, aes(x=fuel, y=seller_type, fill=Count)) +
  geom_tile() +
  theme_bw() + 
  labs(title="Association of fuel and seller_type", subtitle="n=4182") +
  geom_text(aes(label=Count), color="grey54", vjust=1, size=5, fontface="bold") + #data labels
  scale_fill_distiller(palette="Blues", direction=1)  #heatmap color palettes with RColorBrewer

ht1
chi1
```
We see that dealers are a bit more likely to sell diesel cars, while individuals are a bit more likely to sell petrol cars. The association is weak but statistically significant.
\
\
```{r}
tchi2 <- table(df_org$fuel, df_org$transmission)
dfchi2 <- as.data.frame(tchi2)
names(dfchi2)[1] <- "fuel"
names(dfchi2)[2] <- "transmission"
names(dfchi2)[3] <- "Count"
chi2 <- chisq.test(tchi2)

ht2 <- ggplot(dfchi2, aes(x=fuel, y=transmission, fill=Count)) +
  geom_tile() +
  theme_bw() + 
  labs(title="Association of fuel and transmission", subtitle="n=4182") +
  geom_text(aes(label=Count), color="grey54", vjust=1, size=5, fontface="bold") + #data labels
  scale_fill_distiller(palette="Blues", direction=1)  #heatmap color palettes with RColorBrewer

ht2
chi2
```
Automatic cars are a bit more likely to be diesel, and manual cars are a bit more likely to be petrol. Again, a weak but statistically significant association.
\
\
```{r}
tchi3 <- table(df_org$fuel, df_org$owner)
dfchi3 <- as.data.frame(tchi3)
names(dfchi3)[1] <- "fuel"
names(dfchi3)[2] <- "owner"
names(dfchi3)[3] <- "Count"
chi3 <- chisq.test(tchi3)

ht3 <- ggplot(dfchi3, aes(x=fuel, y=owner, fill=Count)) +
  geom_tile() +
  theme_bw() + 
  labs(title="Association of fuel and owner", subtitle="n=4182") +
  geom_text(aes(label=Count), color="grey54", vjust=1, size=5, fontface="bold") + #data labels
  scale_fill_distiller(palette="Blues", direction=1) + 
  scale_y_discrete(limits=rev(levels(df_org$owner)))

ht3
chi3
```
There is no significant association between fuel type and number of owners.
\
\
```{r}
tchi4 <- table(df_org$seller_type, df_org$transmission)
dfchi4 <- as.data.frame(tchi4)
names(dfchi4)[1] <- "seller_type"
names(dfchi4)[2] <- "transmission"
names(dfchi4)[3] <- "Count"
chi4 <- chisq.test(tchi4)

ht4 <- ggplot(dfchi4, aes(x=seller_type, y=transmission, fill=Count)) +
  geom_tile() +
  theme_bw() + 
  labs(title="Association of seller_type and transmission", subtitle="n=4182") +
  geom_text(aes(label=Count), color="grey54", vjust=1, size=5, fontface="bold") + #data labels
  scale_fill_distiller(palette="Blues", direction=1)  #heatmap color palettes with RColorBrewer

ht4
chi4
```
Individual sellers are much more likely to sell manual cars compared to dealers. This is a strong, highly statistically significant association.
\
\
```{r}
tchi5 <- table(df_org$seller_type, df_org$owner)
dfchi5 <- as.data.frame(tchi5)
names(dfchi5)[1] <- "seller_type"
names(dfchi5)[2] <- "owner"
names(dfchi5)[3] <- "Count"
chi5 <- chisq.test(tchi5)

ht5 <- ggplot(dfchi5, aes(x=seller_type, y=owner, fill=Count)) +
  geom_tile() +
  theme_bw() + 
  labs(title="Association of seller_type and owner", subtitle="n=4182") +
  geom_text(aes(label=Count), color="grey46", vjust=1, size=5, fontface="bold") + #data labels
  scale_fill_distiller(palette="Blues", direction=1) +
  scale_y_discrete(limits=rev(levels(df_org$owner)))

ht5
chi5
```
Dealers sell roughly 1/3rd of first owner cars, 1/10th of second owner cars, and 1/33th of third owner cars. There is a very strong and highly statistically significant association between these variables. Dealers strongly prefer selling first owner cars.
\
\
```{r}
tchi6 <- table(df_org$transmission, df_org$owner)
dfchi6 <- as.data.frame(tchi6)
names(dfchi6)[1] <- "transmission"
names(dfchi6)[2] <- "owner"
names(dfchi6)[3] <- "Count"
chi6 <- chisq.test(tchi6)

ht6 <- ggplot(dfchi6, aes(x=transmission, y=owner, fill=Count)) +
  geom_tile() +
  theme_bw() + 
  labs(title="Association of transmission and owner", subtitle="n=4182") +
  geom_text(aes(label=Count), color="grey54", vjust=1, size=5, fontface="bold") + #data labels
  scale_fill_distiller(palette="Blues", direction=1) +
  scale_y_discrete(limits=rev(levels(df_org$owner)))

ht6
chi6
```
First owner cars are a bit more likely to be automatic transmission cars. There is a moderate, statistically significant association between these variables.
\
\
```{r}
for (i in 2:4) {
  for (k in 5:8) {
    grp <- df_org[,k]
    y_val <- df_org[,i]
    x_lab <- colnames(df_org)[k]
    y_lab <- colnames(df_org)[i]
    
      
    p <- ggplot(data=df_org, aes(x=!!grp, y=!!y_val, fill=!!grp)) +
      geom_boxplot(width=0.1, lwd=0.75, fatten=0.75) +
      stat_boxplot(geom="errorbar", width=0.1, lwd=0.75, fatten=0.75) +
      labs(x=x_lab, y=y_lab) +
      theme_bw() +
      theme(legend.position = "none") +
      scale_fill_brewer(palette="Set1")
    
    itr <- paste0(i, k)
    assign(paste0("box", itr), p)
  }
}
```
Finally, let's look at the correlations between numeric and factor variables, using boxplots of numeric variables grouped by factor variables.
\
```{r}
(box35|box36) + plot_annotation(title="Selling price in thousands, grouped by fuel and seller_type", subtitle="N=4,182", theme=theme_bw())

(box37|box38) +  plot_annotation(title="Selling price in thousands, grouped by transmission and owner", subtitle="N=4,182", theme=theme_bw())
```
\

- We see that diesel cars tend to be priced higher than petrol cars.
- Cars sold by dealers tend to be priced a bit higher than cars sold by individuals.
- Automatic transmission cars tend to be priced much higher than manuals.
- The more previous owners a car has, the lower it is priced. The effect is more pronounced for first owner cars, and less so between second and third owner cars.

\
```{r}
(box25|box26) / (box27|box28) + plot_annotation(title="Year, grouped by factor variables", subtitle="N=4,182", theme=theme_bw())
```
\

- Overall, diesel cars tend to be a bit newer than petrol cars, though there are numerous older diesels as well.
- Cars sold by dealers tend to be a bit newer compared to cars sold by individuals.
- Automatic transmission cars tend to be a bit newer compared to manuals.
- Generally, the more previous owners a car has, the older it is.

\
```{r}
(box45|box46) + plot_annotation(title="km_driven in thousands, grouped by fuel and seller type", subtitle="N=4,182", theme=theme_bw())

(box47|box48) + plot_annotation(title="km_driven in thousands, grouped by transmission and owner type", subtitle="N=4,182", theme=theme_bw())

```
\

- Diesel cars tend to have higher kilometers driven compared to petrols. This is expected as diesel engines are known to be more durable.
- Cars sold by individuals tend to have higher kilometers driven compared to cars sold by dealers.
- Manual transmission cars tend to have higher kilometers driven compared to automatics.
- Generally, cars with more previous owners tend to have higher kilometers driven, though there are some first owner cars with the highest kilometers driven among the cars in the dataset.

### Relationships with selling price

Now let's consider the relationships between selling price, and the other variables in our dataset. We have already done this to a degree with our correlations analysis above. For factor variables, we can say:

- Fuel type has a moderate relationship with selling price. Diesel cars are more expensive than petrol cars.
- Transmission type has a strong relationship with selling price. Automatic cars tend to be more expensive, though we have few observations with automatic transmissions..
- Previous owners has a moderate relationship with selling price. First owner cars tend to be more expensive, but the effect for second and third owner cars is less clear, as there are few observations with these levels.
- Seller type has a somewhat weaker relationship with selling price. Cars sold by dealers tend to be priced a bit higher.

```{r}
for (i in c(2,4)) {
  for (k in 5:8) {
    grp <- df_org[,k]
    legnm <- colnames(df_org)[k]
    xval <- df_org[,i]
      
    p <- ggplot(data=df_org, aes(x=!!xval, y=df_org[,3], color=!!grp)) +
      geom_point(stat="identity") +
      labs(x=colnames(df_org[i]), y="selling_price", color=legnm) +
      theme_bw()
    
    itr <- paste0(i, k)
    assign(paste0("plt", itr), p)
  }
}
```
Let's use scatterplots to better visualize the relationship between the two numeric predictors, year and km_driven, and the selling prices. Let's also group the observations by the factor variables for more possible insights.
\
```{r}
plt_1 <- (plt25 | plt45) / (plt26 | plt46) + plot_layout(guides="collect" )  + plot_annotation(title="Numeric variables versus selling price, grouped by fuel and seller type", subtitle="N=4,182, selling_price and km_driven in thousands", theme=theme_bw())
plt_1

```
\
```{r}
(plt27 | plt47) / (plt28 | plt48) + plot_layout(guides="collect" ) + plot_annotation(title="Numeric variables versus selling price, grouped by transmission and owner type", subtitle="N=4,182, selling_price and km_driven in thousands", theme=theme_bw())
```
\

- Selling price tends to increase with year, in a non-linear, exponential-like fashion.
  - Diesels, automatics, dealer cars and first owner cars all tend to be newer, and higher priced. 
- Selling price doesn't have a clear relationship with kilometers driven, though there are no cars priced above roughly 1.25m with more than 200k kilometers driven.
  - Automatics, dealer cars and first owner cars all tend to have lower kilometers.

## OLS Linear Regression

Before we fit an OLS linear regression model, there are a few transformations we need to consider:

- Our outcome variable, selling price, follows a non-normal distribution. We will use the log transformed version as the outcome variable in our regression, to try and satisfy the normality of residuals assumption.
- The relationship between year and selling price is likely exponential and not linear. Since we will apply a log transformation to selling price, we don't need to apply an exponential transformation to year.

```{r}
df <- df_org
```
As for variable choice, we saw that year, fuel type, transmission type and owner type are all potential predictors of selling price. There is no apparent strong correlation within these variables, so we can include all of them as predictors.
\
\
Kilometers driven and seller type exhibit weak relationships with selling price. Besides, they may be largely caused by the other variables in our dataset:

- Kilometers driven is correlated / associated with year and fuel type. Diesel engines tend to last longer than petrol engines, and intuitively we would expect older cars to be driven for longer distances. We can consider km_driven largely as a consequence of fuel type and year, rather than a separate predictor.
- Similarly, seller type is associated with transmission type and owner type. It's likely that dealers prefer automatic, first owner cars, as well as possibly newer, automatic and lower kilometer cars. Again, we can consider dealer type as a consequence of the other predictors in our dataset.
- These variables are largely caused by the other predictors, while not being significant predictors of the outcome variable themselves. Including them in our model would lead to selection bias: Some of the effects of the true predictors on the outcome variable would be held constant, leading to misleading results about the effects of our true predictor variables.

Therefore, we will keep kilometers driven and seller type out of our regression model.

### lm1 results

Let's fit our first linear model, with the log transformed selling price as the outcome variable, and year, fuel, transmission, owner as our predictor variables.
```{r, echo=TRUE}
lm1 <- lm(log(selling_price) ~ year + fuel + transmission + owner, data=df)
summary(lm1)
```

- Our model explains 67% of the variance in the selling prices.
- All predictors are highly significant, with p-values close to zero.
- Year has a positive effect on price, while the other predictors all have negative effects.
- The intercept is -229, which is very close to 0 when we reverse the log transformation.

Since the coefficients are log transformed, let's reverse the transformations, convert them into percentage changes, and get the 95% confidence intervals.
```{r}
lm1_coefs <- lm1$coefficients
lm1_coefs <-lm1_coefs[2:6]
lm1_coefs <- (exp(lm1_coefs) - 1) * 100

lm1_conf95 <- confint(lm1, level=0.95)
lm1_conf95 <- as.data.frame(lm1_conf95)
lm1_conf95 <- lm1_conf95[2:6,]
lm1_conf95 <- (exp(lm1_conf95) - 1) * 100

lm1_conf95 <- lm1_conf95 %>% mutate(lm1_conf95, estimate=lm1_coefs, .before="97.5 %")
lm1_conf95
```
Here's how to interpret these transformed coefficient estimates, and the confidence intervals:

- Increasing year by 1 is expected to increase price by 12.44%, holding everything else constant. The 95% confidence interval is 12-13%: There is a 95% chance the true mean of the coefficient translates into a 12-13% increase in price.
- A petrol car is expected to sell for 40% less compared to a diesel car.
- A manual transmission car is expected to sell for 57% less compared to an automatic.
- A second owner car is expected to sell for 8% less, and a third owner car for 15% less, compared to a first owner car.

Another way to visualize the individual effect of each variable on price, while holding the effects of other variables constant, is with added variable plots:
\
```{r}
avPlots(lm1)
```

- Year has the biggest effect on price: A positive, approximately linear increase. The log transformation of selling price apparently worked well to capture the relationship between year and selling price.
  - Year had the second smallest coefficient, but this can be misleading: The coefficients are unit effects on price, and year is a numeric variable with a range of values from 1995-2020. The other variables are factor variables with 2 or 3 levels. Therefore, year has more potential to affect the price. 
    - For example, going from a year value of 1995 to 2020, while keeping other predictors constant, we can expect the new price to be (1.12)^25 times the old one. That is 17 times the original price!
- Transmission has the second biggest effect on price: A decline for manual transmission cars, or an increase for automatics.
- Then fuel: A decline for petrol cars, or an increase for diesels.
- Finally owner type: A small decline for second owners, and a slightly bigger decline for third owners.

### lm1 diagnostics

Let's plot the diagnostic graphs for lm1, and see if our model fits the assumptions for an OLS linear regression.
\
```{r}
autoplot(lm1)
vif(lm1)
```

- The linearity assumption is generally satisfied, as there are no strong patterns in the residuals vs fitted plot.
- The normality of residuals assumption generally holds, but the residuals for the highest quantiles deviate from normality, which is likely because of the higher priced, luxury cars in our dataset not being predicted well.
- The equal variance assumption is generally satisfied, as there are no strong patterns in the scale-location plot. However, the residuals for the highest fitted values are considerably higher, again likely due to the high-priced luxury cars being poorly predicted.
- There are no particularly high leverage observations present. There are a few outliers in terms of standardized residuals.
- The variance inflation factors for all variables are between 1-1.3, meaning there are no multicollinearity issues present in our model.

Another way to visually assess the model's fit is to plot the fitted values against the real observed values. Let's do this, while reversing the log transformations on the model's predictions.
\
```{r}
fit_lm1 <- exp(fitted(lm1))
df_fit_lm1 <- data.frame(fitted=fit_lm1, observed=df_org$selling_price)

p_fit_lm1 <- ggplot(df_fit_lm1, aes(x=fitted, y=observed)) + geom_point(color="#0571B0") + theme_bw() + 
  geom_abline(intercept=0, slope=1, color="#CA0020", size=1) + 
  labs(title="Selling prices in thousands, predicted by lm1 vs. real", subtitle="N=4182, red straight line for reference", x="Predicted selling prices",
       y="Real selling prices") 
p_fit_lm1
```
\
While the fitted vs. observed values are reasonably close to following a straight line, quite a few observations, especially higher priced cars above 2.5m, are vastly underpredicted. We expected this from our exploratory analysis, and splitting the data can offer more accurate predictions, but for the sake of our example, let's stick with these results.

## Bayesian Linear Regression

Now, let's fit a BLR model with the same formula as lm1. We will go over the outputs of a BLR model, and compare them with the outputs of our OLS model lm1. Our outcome variable is log transformed, which leads to a distribution reasonably close to a standard normal distribution, so we will leave the default setting of a normal distribution as the prior distribution. We don't have any prior information on the regression coefficients' distributions, so we will leave them default too.

### blr1 results

Our BLR model is constructed with a Markov Chain Monte Carlo algorithm, which draws random samples from the model parameters' distribution. Each iteration of a BLR model will yield different results, so we need to set seed to get replicable outputs.
```{r, echo=TRUE, results='hide'}
set.seed(1)
blr1 <- stan_glm(log(selling_price) ~ year + fuel + transmission + owner, 
                 data=df, seed=1)
```
The coefficients of the model blr1 are log transformed, so let's reverse the transformations, convert them into percentage changes, and see the posterior distributions of the model parameters.
```{r}
param_blr1 <- describe_posterior(blr1)
param_blr1 <- param_blr1[2:6,]

for (i in c(2,4,5,8,9)){
  param_blr1[,i] <- (exp(param_blr1[,i]) - 1) * 100
}

param_blr1
```
Let's compare these with the estimated coefficients of lm1, and their 95% confidence intervals.
```{r}
lm1_conf95
```

We can see that the estimates of lm1 and blr1 are very close. The 95% confidence intervals of lm1 are also very close to the 95% credible intervals of blr1. However, their interpretations are a bit different: 

- The estimates of lm1 are estimated means of the regression coefficients, while the estimates of blr1 are the estimated medians. 
- With an OLS linear regression model, there is a 95% chance that the true mean of the coefficient lies within the 95% confidence interval.
- With a BLR model, 95% of all coefficient estimates lie within the 95% credible interval.

We also have a few metrics to assess the accuracy of each parameter in blr1:

- pd is the probability that the direction of the coefficient is true. This is 100% for all our coefficients.
- ROPE is a range close to zero, which is considered practically zero. If the credible interval for a coefficient is fully covered by the ROPE, we can consider the effect of that variable to be practically zero. All the coefficients in our model are fully outside the ROPE.
  - The interpretation of "% in ROPE" is somewhat similar to the p-value in OLS: x% in ROPE means there is a x% chance that the effect is not significant.
- Rhat is the scale reduction factor. If it is close to 1, the MCMC algorithm came close to reaching convergence. In our case, all Rhat values are 1 or very close.
- ESS is effective sample saze, the number of independent draws that give us the same information as the MCMC algorithm. Generally, a high ESS, higher than 400, means the MCMC was informative. In our case, the lowest ESS is 4329.

```{r, include=FALSE}
color_scheme_set("teal")

post_year <- mcmc_areas(blr1, pars = c("year"), prob=0.95,
                        point_est=c("median"), transformations="exp", border_size=1.1)

post_year <- post_year + theme_bw() 

post_tr <- mcmc_areas(blr1, pars = c("transmissionManual"), prob=0.95,
                        point_est=c("median"), transformations="exp", border_size=1.1)
post_tr <- post_tr  + theme_bw() + scale_x_continuous(breaks=seq(0.4, 0.45, 0.01))

post_fuel <- mcmc_areas(blr1, pars = c("fuelPetrol"), prob=0.95,
                        point_est=c("median"), transformations="exp", border_size=1.1)
post_fuel <- post_fuel  + theme_bw() + scale_x_continuous(breaks=seq(0.58,0.63,0.01))

post_second <- mcmc_areas(blr1, pars = c("ownerSecond Owner"), prob=0.95,
                        point_est=c("median"), transformations="exp", border_size=1.1)
post_second <- post_second  + theme_bw() + scale_x_continuous(breaks=seq(0.88, 0.98, 0.02))

post_third <- mcmc_areas(blr1, pars = c("ownerThird Owner"), prob=0.95,
                        point_est=c("median"), transformations="exp", border_size=1.1)
post_third <- post_third  + theme_bw() + scale_x_continuous(breaks=seq(0.78, 0.92, 0.02))
  
```

### Posterior probability distributions

BLR generates a posterior probability distribution for the model parameters, which can help visualize their level of uncertainty. Let's plot the posterior distributions of our model coefficients, with the log transformations reversed. Keep in mind that the X-axis values in the below plots are just exponents of the log coefficients. The closer they are to 1, the smaller their effect on price. A coefficient greater than 1 has a positive effect on price, and smaller than 1 has a negative effect on price. 
\
\
Let's plot the posterior probability distributions of the model coefficients.
\
```{r}
post_year / post_tr + plot_annotation(title="Posterior distributions of the year and manual transmission coefficients", subtitle="Area: 95% credible interval. Line: median estimate. X-axes: exponential of the coefficients")
```
\
```{r}
post_fuel + labs(title="Posterior distribution of the petrol fuel coefficient", subtitle="Area: 95% credible interval. Line: median estimate", x="Exponential values of the coefficient") 
```
\
```{r}
post_second / post_third + plot_annotation(title="Posterior distributions of the second and third owner coefficients", subtitle="Area: 95% credible interval. Line: median estimate. X-axes: exponential of the coefficients")
```
\
\
If we want to compare the effect and uncertainty of each coefficient, we can plot their distributions together, though it will be hard to see the distribution for each individual coefficient this way.
\
```{r, include=FALSE}
post_all <- mcmc_areas(blr1, pars = c("year", "fuelPetrol", "transmissionManual",
"ownerSecond Owner", "ownerThird Owner"), prob=0.95, point_est=c("median"), 
transformations="exp") 

post_all <- post_all + labs(title="Posterior distributions of all coefficients", subtitle="Area: 95% credible interval. Line: median estimate", x="Exponential values of the coefficients")  + theme_bw() 
```

```{r}
post_all
```
\
In this plot, coefficients that are farther away from 1 have a bigger unit effect on the price, coefficients greater than 1 have a positive effect, and coefficients smaller than 1 have a negative effect.

- Manual transmission has the biggest unit effect on price, with a negative effect.
- Petrol fuel follows with a negative effect.
- Third owner and second owner coefficients have smaller negative effects on price, in that order of effectiveness.
- Year has the second smallest unit effect on the price, after second owner. 
  - Again, keep in mind that year is numeric and its values range from 1995 to 2020, and the other predictors are all factor predictors with 2-3 levels, so year has a bigger potential effect on price overall.

The ranges of the coefficient posterior distributions give us an idea about their uncertainty.

- Year's coefficient has the narrowest range, which means the effect of this variable has the least amount of uncertainty. 
- In contrast, third owner's coefficient has the widest range, making it the variable with the most uncertain effect. 
- This is expected, as there are very few observations of the second and third owner types.

The BLR model also gives us a posterior probability distribution for the predicted values of the outcome variable. We can plot this against the distribution of the original selling price in our dataset, to check if the predictions fit well with the original data.
\
```{r}
pp_blr1 <- pp_check(blr1, seed=1)
pp_blr1 <- pp_blr1 + theme_bw() + labs(x="log(selling_price)")

pp_blr1_exp <- pp_blr1
pp_blr1_exp$data$value <- exp(pp_blr1_exp$data$value)
pp_blr1_exp <- pp_blr1_exp + theme_bw() + labs(x="selling_price, in thousands")

(pp_blr1 / pp_blr1_exp) + plot_layout(guides="collect") + plot_annotation(title="Probability distributions of selling_price", subtitle="y=Original distribution, yrep=Posterior distributions predicted by blr1")
```
\
Overall, the posterior probability distributions are reasonably close to the distribution of the original outcome variable, though there are considerable deviations in certain ranges. We can see from these plots that the log transformation brought the distribution of selling price reasonably close to a normal distribution.

### Predicted vs real values plot

We can plot the average selling prices predicted by blr1, against the actual values of selling price, just like we did with lm1. This will yield very similar results, as the average estimates of blr1 are very close to the estimates of lm1.
\
```{r}
fit_blr1 <- exp(fitted(blr1))
blr1_name <- "blr1"
lm1_name <- "lm1"
df_fit_blr1 <- data.frame(fitted=fit_blr1, observed=df_org$selling_price, model=blr1_name)
df_fit_lm1$model <- lm1_name
df_fit_blr1 <- rbind(df_fit_blr1, df_fit_lm1)

p_fit_blr1 <- ggplot(df_fit_blr1, aes(x=fitted, y=observed, color=model)) + geom_point() + theme_bw() + 
  geom_abline(intercept=0, slope=1, color="#CA0020", size=1) + 
  labs(title="Selling prices in thousands, predicted vs. real", subtitle="N=4182", x="Predicted selling prices",
       y="Real selling prices") 
p_fit_blr1
```
\
We can see that the average fitted values of blr1 are practically the same with the fitted values of lm1. They overlap almost perfectly in the plot, though blr1's predictions are slightly higher.

### Posterior predictive distribution vs. OLS prediction

While the OLS model lm1 generates one prediction per one data point, the BLR model blr1 generates 4,000 predictions per data point. We can visualize the range of predictions made by blr1 as a histogram, and overlay the single prediction made by lm1. Let's do this for one observation in our dataset:
\
```{r}
set.seed(1)
df_pred <- df[sample(nrow(df), 1),]
df_pred
```
Let's predict a single price for this car, using lm1, and 4,000 prices, using blr1. Let's plot the prices predicted by blr1 as a histogram, along with the point prediction of lm1, and the original price of the car.
```{r}
set.seed(1)
pred_lm1 <- as.data.frame(exp(predict(lm1, newdata=df_pred)))
pred_blr1 <- as.data.frame(exp(posterior_predict(blr1, newdata=df_pred)))
```

```{r}
colnames(pred_lm1)[1] <- "lm1"
colnames(pred_blr1)[1] <- "blr1"
```

\
```{r}
hist_pred <- ggplot(data=pred_blr1, aes(x=blr1)) +
                      geom_histogram(alpha=0.75, bins=30, position="identity", aes(fill="#92C5DE"), color="#92C5DE", size=1) +
                      geom_point(aes(x=pred_lm1$lm1, y=440, color="#CA0020"), size=2) +
                      geom_point(aes(x=df_pred$selling_price, y=440, color="#018571"), size=2) +
                      geom_point(aes(x=quantile(pred_blr1$blr1,0.25), y=440, color="#FDB863"), size=2) +
                      geom_point(aes(x=quantile(pred_blr1$blr1,0.75), y=440, color="#FDB863"), size=2) +
                      geom_vline(xintercept=pred_lm1$lm1, color="#CA0020", size=1, linetype="dashed") +
                      geom_vline(xintercept=df_pred$selling_price, color="#018571", size=1, linetype="dashed") +
                      geom_vline(xintercept=quantile(pred_blr1$blr1,0.25), color="#FDB863", size=1, linetype="dashed") + geom_vline(xintercept=quantile(pred_blr1$blr1,0.75), color="#FDB863", size=1, linetype="dashed") +
                      labs(title="Price of one car from our dataset", 
                           subtitle="As predicted by blr1 & lm1, along with the real price",
                           x="Selling price, thousands", y="Count") +
  scale_color_identity(guide="legend",
                       name="",
                       breaks=c("#CA0020", "#018571", "#FDB863"),
                       labels=c("lm1 point prediction", "Real price", "1st-3rd quartiles of\n blr1 predictions")) +
  scale_fill_identity(guide="legend",
                       name="",
                      breaks=c("#92C5DE"),
                      labels=c("blr1 predictions")) + theme_bw() + scale_x_continuous(breaks=seq(0,2500,250))

hist_pred + theme(plot.margin=unit(c(5,5,5,5),"mm"))
```
\
lm1's point prediction for this observation is 390k, underestimating the real price of 500k. However, blr1's posterior prediction tells us more about the degree of uncertainty in the predictions of price:

- Roughly 50% of blr1's predictions fall between 250k and 650k, with the median prediction of 385k. This shows us that blr1 tends to underestimate the price of this particular car.
- The full range of blr1's predictions for this car goes as low as 70k, and as high as 2.4m.
- Just like the distribution of selling prices in the original dataset, the posterior predictive distribution is very right-skewed with large outliers.

## Conclusions

Our OLS linear regression model, lm1, yielded a decent fit for our data, generally satisfying the assumptions of OLS linear regression, or coming close.

- The model explained 67% of the variance in used car selling prices, using the highly significant predictor variables year, fuel type, transmission type and owner type.
- The model is prone to underpredicting the price of most higher-priced cars, especially those with selling prices in millions. 

Our BLR model, blr1, yielded average estimates for model parameters and average predictions close to those of lm1.

- However, we were able to get a better understanding of the uncertainty in our model, thanks to the posterior distributions generated by blr1.
- blr1 identified transmission type as the predictor with the strongest unit effect on selling price, while the coefficient for year had the lowest uncertainty, and the coefficient for third owner type had the highest uncertainty.
- The posterior predictive distributions from our model, just like the original selling price variable, follow a very right-skewed distribution, with large outliers.

Overall, both models confirmed the predictor variables used as very significant predictors of selling price, but the overall predictions can be very inaccurate, especially for higher priced cars.

- Since we would expect different price dynamics for different segments of cars, removing the high-priced segment and predicting the prices of "ordinary" cars is likely to yield more accurate predictions. 
- However, with less data points for the model, we can expect a lower amount of variance explained. There are likely other significant factors at play in used car prices, and without bringing them into our model, we can't improve our predictions by a lot. 
